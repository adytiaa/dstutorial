{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Introduction\n",
    "\n",
    "## Details\n",
    "\n",
    "Machine Learning – Problemstellung und Lösungen Einführung in das Python Paket „Pandas“ Das Einlesen von Daten Die Analyse von Daten Datenmanipulationen Machine Learning Grundlagen Die Unterschiede zwischen Überwachtem und Unüberwachtem Lernen Vorstellung verschiedener Modelle, wie Lineare Regression, Logistische Regression k-means Clustering K-nearest neighbours Die Validierung von Modellen (Cross Validation)\n",
    "\n",
    "* There is much debate among scholars and practitioners about what data science is, and what it isn’t. Does it deal only with big data? What constitutes big data? Is data science really that new? How is it different from statistics and analytics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial Outline\n",
    "\n",
    "### 1. Pandas and Visualisation: Data Analysis\n",
    "\n",
    "### 2. Machine learning in more detail\n",
    "\n",
    "### 3. Evaluation of models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Day 3\n",
    "\n",
    "### Machine learning: a detailed introduction\n",
    "\n",
    "### Using scikit-learn to do machine learning\n",
    "\n",
    "### Evaluation techniques\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# General Outline\n",
    "\n",
    "### What is machine learning?\n",
    "    \n",
    "###   Supervised and Unsupervised learning\n",
    "    \n",
    " ###  Scikit-Learn for Machine Learning \n",
    "    \n",
    "###    Model evaluation and Validation\n",
    "        \n",
    "###    Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "1. Introduction to machine learning\n",
    "\n",
    "    Supervised vs. unsupervised learning\n",
    "    Examples of applications of supervised and unsupervised learning\n",
    "    Pattern recognition cycle\n",
    "\n",
    "2. Approaches to supervised learning:\n",
    "\n",
    "Methods that estimate probability density functions explicitly\n",
    "\n",
    "    Parametric methods: Quadratic Discriminant Analysis, Linear Discriminant Analysis, Diagonal Linear Discriminant Analysis\n",
    "    Non-parametric methods: Histogram method, k-Nearest Neighbor\n",
    "\n",
    "Methods that estimate decision boundaries directly:                                      \n",
    "\n",
    "    Linear: Logistic discrimination, support vector machines\n",
    "    Nonlinear – neural networks (multilayer perceptron)\n",
    "    Decision trees and random forest\n",
    "\n",
    "3. Performance assessment:\n",
    "\n",
    "    Metrics: Accuracy, Area Under Receiver Operating Characteristic curve (ROC)\n",
    "    Methods: hold-out, leave-one out cross-validation, N-fold cross-validation\n",
    "\n",
    "4. Feature selection:\n",
    "\n",
    "    Filter\n",
    "    Wrapper\n",
    "    Search algorithms: Best Individual N, Forward selection, Backward deletion, Combinatorial\n",
    "\n",
    "5. Feature extraction: Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning - Introduction\n",
    "\n",
    "### It is a scientific discipline concerned with the design and development of Algorithms that allow computers to evolve behaviors based on empirical data, such as from sensor data or databases.\n",
    "\n",
    "### A learner can take advantage of examples (data) to capture characteristics of interest of their unknown underlying probability distribution. Data can be seen as examples that illustrate relations between observed variables.\n",
    "\n",
    "### A major focus of machine learning research is to automatically learn to recognize complex patterns and make intelligent decisions based on data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "### Definition: Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.\n",
    "\n",
    "\n",
    "### Definition by prof. Tom Mitchell: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n",
    "\n",
    "\n",
    "### Machine Learning is about building programs with tunable parameters (typically an array of floating point values) that are adjusted automatically so as to improve their behavior by adapting to previously seen data.\n",
    "\n",
    "### Machine Learning can be considered a subfield of Artificial Intelligence since those algorithms can be seen as building blocks to make computers learn to behave more intelligently by somehow generalizing rather that just storing and retrieving data items like a database system would do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# What is Machine Learning?\n",
    "\n",
    "## One more definition: \"Machine learning is the semi-automated extraction of knowledge from data\"\n",
    "\n",
    "\n",
    "## Knowledge from data: Starts with a question that might be answerable using data\n",
    "\n",
    "## Automated extraction: A computer provides the insight\n",
    "\n",
    "## Semi-automated: Requires many smart decisions by a human\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions about machine learning\n",
    "\n",
    "    How do I choose which attributes of my data to include in the model?\n",
    "    How do I choose which model to use?\n",
    "    How do I optimize this model for best performance?\n",
    "    How do I ensure that I'm building a model that will generalize to unseen data?\n",
    "    Can I estimate how well my model is likely to perform on unseen data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When Do We Need Machine Learning?\n",
    "\n",
    "## When do we need machine learning rather than directly program our computers to carry out the task at hand? \n",
    "\n",
    "* Two aspects of a given problem may call for the use of programs that learn and improve on the basis of their “experience”: the problem’s complexity and the need for adaptivity.\n",
    "\n",
    "### Tasks That Are Too Complex to Program.\n",
    "\n",
    "* Tasks Performed by Animals/Humans: There are numerous tasks that we human beings perform routinely, yet our introspection concerning how we do them is not sufficiently elaborate to extract a well defined program\n",
    "\n",
    "* Tasks beyond Human Capabilities: Another wide family of tasks that benefit from machine learning techniques are related to the analysis of very large and complex data sets: astronomical data, turning medical archives into medical knowledge, weather prediction, analysis of genomic data, Web search engines, and electronic commerce.\n",
    "\n",
    "* With more and more available digitally recorded data, it becomes obvious that there are treasures of meaningful information buried in data archives that are way too large and too complex for humans to make sense of.\n",
    "\n",
    "### Adaptivty\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does machine learning \"work\"?\n",
    "\n",
    "## High-level steps of supervised learning:\n",
    "\n",
    "  * First, train a machine learning model using labeled data\n",
    "  * \"Labeled data\" has been labeled with the outcome\n",
    "  * \"Machine learning model\" learns the relationship between the attributes of the data and its outcome\n",
    "  * Then, make predictions on new data for which the label is unknown\n",
    "\n",
    "Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Popular Libraries for ML \n",
    "\n",
    "### Scikit-Learn \n",
    "\n",
    "### Apache Singa \n",
    "\n",
    "### Amazon Machine Learning  \n",
    "\n",
    "### Azure ML Studio\n",
    "\n",
    "### MLlib (Spark)\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "### Torch and PyTorch (machine learning algorithms that puts GPUs first\n",
    "\n",
    "### many other \n",
    "\n",
    "![](./assest/mlframe.jpg?raw=true)\n",
    "#### Source: KDnuggets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setting up Python for ML: scikit-learn and IPython Notebook\n",
    "    \n",
    "### key points\n",
    "\n",
    "    Features of scikit-learn?\n",
    "    \n",
    "    How do I install scikit-learn?\n",
    "    \n",
    "    How do I use the IPython Notebook?\n",
    "    What are some good resources for learning Python?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scikit-Learn\n",
    "\n",
    "## Scikit-Learn is a Python package designed to give access to well-known machine learning algorithms within Python code, through a clean, well-thought-out API. \n",
    "\n",
    "## It has been built by hundreds of contributors from around the world, and is used across industry and academia.\n",
    "\n",
    "\n",
    "## scikit-Learn is built upon Python's NumPy (Numerical Python) and SciPy (Scientific Python) libraries, which enable efficient in-core numerical and scientific computation within Python.\n",
    "\n",
    "## scikit-learn is not specifically designed for extremely large datasets, though there is some work in this area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducing Scikit-Learn\n",
    "\n",
    "### Python libraries which provide solid implementations of a range of machine learning algorithms. One of the best known is Scikit-Learn, a package that provides efficient versions of a large number of common algorithms. \n",
    "\n",
    "### Scikit-Learn is characterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation. A benefit of this uniformity is that once you understand the basic use and syntax of Scikit-Learn for one type of model, switching to a new model or algorithm is very straightforward.\n",
    "\n",
    "### This section provides an overview of the Scikit-Learn API; a solid understanding of these API elements will form the foundation for understanding the deeper practical discussion of machine learning algorithms and approaches in the following chapters.\n",
    "\n",
    "### We will start by covering data representation in Scikit-Learn, followed by covering the Estimator API, and finally go through a more interesting example of using these tools for exploring a set of images of hand-written digits.\n",
    "\n",
    "### Data Representation in Scikit-Learn\n",
    "\n",
    "### Machine learning is about creating models from data: for that reason, we'll start by discussing how data can be represented in order to be understood by the computer. The best way to think about data within Scikit-Learn is in terms of tables of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scikit-Learn's Estimator API\n",
    "\n",
    "### The Scikit-Learn API is designed with the following guiding principles in mind, as outlined in the Scikit-Learn API paper:\n",
    "\n",
    "    * Consistency: All objects share a common interface drawn from a limited set of methods, with consistent documentation.\n",
    "\n",
    "    * Inspection: All specified parameter values are exposed as public attributes.\n",
    "\n",
    "    * Limited object hierarchy: Only algorithms are represented by Python classes; datasets are represented in standard formats (NumPy arrays, Pandas DataFrames, SciPy sparse matrices) and parameter names use standard Python strings.\n",
    "\n",
    "    * Composition: Many machine learning tasks can be expressed as sequences of more fundamental algorithms, and Scikit-Learn makes use of this wherever possible.\n",
    "\n",
    "    * Sensible defaults: When models require user-specified parameters, the library defines an appropriate default value.\n",
    "\n",
    "### In practice, these principles make Scikit-Learn very easy to use, once the basic principles are understood. \n",
    "### Every machine learning algorithm in Scikit-Learn is implemented via the Estimator API, which provides a consistent interface for a wide range of machine learning applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of the API\n",
    "\n",
    "## Most commonly, the steps in using the Scikit-Learn estimator API are as follows\n",
    "\n",
    "    Choose a class of model by importing the appropriate estimator class from Scikit-Learn\n",
    "    \n",
    "    Choose model hyperparameters by instantiating this class with desired values\n",
    "    \n",
    "    Arrange data into a features matrix and target vector following the discussion above\n",
    "    \n",
    "    Fit the model to your data by calling the fit() method of the model instance\n",
    "    \n",
    "    Apply the Model to new data:\n",
    "        For supervised learning, often we predict labels for unknown data using the predict() method.\n",
    "        For unsupervised learning, we often transform or infer properties of the data using the transform() or predict() method.\n",
    "\n",
    "## several simple examples of applying supervised and unsupervised learning methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML Algorithms\n",
    "\n",
    "### Supervised \n",
    "     ### 1. Regression examples Linear and logistic regression\n",
    "     ### 2. Classification examples Support Vector machines\n",
    "     \n",
    "### Unsupervised\n",
    "     ### 1. Cluster analysis\n",
    "     ### 2. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Machine Learning Algorithms.\n",
    "\n",
    "    Linear Regression\n",
    "    Logistic Regression\n",
    "    Decision Trees\n",
    "    KNN (K- Nearest Neighbours)\n",
    "    K-Means\n",
    "    Naïve Bayes\n",
    "    Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# what is supervised and unsupervised learning\n",
    "\n",
    "\n",
    "#### Supervised learning: \n",
    "The task of inferring a function from labeled training data.The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and the desired output value (also called the supervisory signal).\n",
    "\n",
    "A supervised learning algorithm analyzes the training data and produces an inferred function, which can used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a “reasonable” way.\n",
    "\n",
    "#### Unsupervised Learning: \n",
    "\n",
    "In data mining or even in data science world, the problem of an unsupervised learning task is trying to find hidden structure in unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised learning\n",
    "\n",
    "## Machine learning task of predicting an output variable \n",
    "from high-dimensional observations in order for classification \n",
    "\n",
    "### classification and regression\n",
    "\n",
    "\n",
    "![](./assest/s12up.png?raw=true)\n",
    "#### Source: http://ipython-books.github.io/featured-04/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "## The set of data (training data) consists of a set of input data and correct responses corresponding to every piece of data.\n",
    "\n",
    "## Based on this training data, the algorithm has to generalize such that it is able to correctly (or with a low margin of error) respond to all possible inputs.\n",
    "\n",
    "## In essence: The algorithm should produce sensible outputs for inputs that weren't encountered during training.\n",
    "\n",
    "## Also called learning from exemplars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression models\n",
    "\n",
    "\n",
    "### Linear Regression Model\n",
    "\n",
    "Linear regression is an attractive model because the representation is so simple.\n",
    "\n",
    "### The representation is a linear equation that combines a specific set of input values (x) the solution to which is the predicted output for that set of input values (y). As such, both the input values (x) and the output value are numeric.\n",
    "\n",
    "\n",
    "\n",
    "For example, in a simple regression problem (a single x and a single y), the form of the model would be:\n",
    "\n",
    "$$ Y = B_0 + {B_1}*X $$\n",
    "\n",
    "\n",
    "￼￼$${\\displaystyle y_{i}=\\beta _{0}1+\\beta _{1}x_{i1}+\\cdots +\\beta _{p}x_{ip}+\\varepsilon _{i}=\\mathbf {x} _{i}^{\\top }{\\boldsymbol {\\beta }}+\\varepsilon _{i},\\qquad i=1,\\ldots ,n,}$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ordinary least squares (OLS) \n",
    "is the simplest and thus most common estimator. It is conceptually simple and computationally straightforward. OLS estimates are commonly used to analyze both experimental and observational data.\n",
    "The OLS method minimizes the sum of squared residuals, and leads to a closed-form expression for the estimated value of the unknown parameter β:\n",
    "$${\\displaystyle {\\hat {\\boldsymbol {\\beta }}}=(\\mathbf {X} ^{\\top }\\mathbf {X} )^{-1}\\mathbf {X} ^{\\top }\\mathbf {y} =\\left(\\sum \\mathbf {x} _{i}\\mathbf {x} _{i}^{\\top }\\right)^{-1}\\left(\\sum \\mathbf {x} _{i}y_{i}\\right).} {\\displaystyle {\\hat {\\boldsymbol {\\beta }}}=(\\mathbf {X} ^{\\top }\\mathbf {X} )^{-1}\\mathbf {X} ^{\\top }\\mathbf {y} =\\left(\\sum \\mathbf {x} _{i}\\mathbf {x} _{i}^{\\top }\\right)^{-1}\\left(\\sum \\mathbf {x} _{i}y_{i}\\right).}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "### Linear Regression Example\n",
    "\n",
    "This example uses the only the first feature of the diabetes dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.\n",
    "\n",
    "The coefficients, the residual sum of squares and the variance score are also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Regression on Housing data USA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic regression: formulation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes)\n",
    "\n",
    "In logistic regression, the dependent variable is binary or dichotomous, i.e. it only contains data coded as 1 (TRUE, success, pregnant, etc.) or 0 (FALSE, failure, non-pregnant, etc.).\n",
    "\n",
    "The goal of logistic regression is to find the best fitting (yet biologically reasonable) model to describe the relationship between the dichotomous characteristic of interest (dependent variable = response or outcome variable) and a set of independent (predictor or explanatory) variables. \n",
    "\n",
    "Logistic regression generates the coefficients (and its standard errors and significance levels) of a formula to predict a logit transformation of the probability of presence of the characteristic of interest:\n",
    "￼\n",
    "\n",
    "where p is the probability of presence of the characteristic of interest. The logit transformation is defined as the logged odds:\n",
    "￼\n",
    "\n",
    "and\n",
    "￼\n",
    "\n",
    "Rather than choosing parameters that minimize the sum of squared errors (like in ordinary regression), estimation in logistic regression chooses parameters that maximize the likelihood of observing the sample values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definition of the logistic function\n",
    "\n",
    "An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic fun￼ction is useful because it can take any real input t, $$( {\\displaystyle t\\in \\mathbb {R} }￼),$$  \n",
    "￼\n",
    "$$\\sigma (t)={\\frac {e^{t}}{e^{t}+1}}={\\frac {1}{1+e^{-t}}}$$\n",
    "\n",
    "whereas the output always takes values between zero and one and hence is interpretable as a probability. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the cross- entropy loss if the ‘multi_class’ option is set to ‘multinomial’. (Currently the ‘multinomial’ option is supported only by the ‘lbfgs’, ‘sag’ and ‘newton-cg’ solvers.)\n",
    "\n",
    "This class implements regularized logistic regression using the ‘liblinear’ library, ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).\n",
    "\n",
    "The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization with primal formulation. The ‘liblinear’ solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression 3-class Classifier\n",
    "\n",
    "Show below is a logistic-regression classifiers decision boundaries on the iris dataset. The datapoints are colored according to their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised learning: Classification\n",
    "    \n",
    "### Support Vector machines\n",
    "\n",
    "### Random forests\n",
    "\n",
    "### Naive Bayes Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Application of Classification Algorithms\n",
    "\n",
    "### Email spam classification\n",
    "### Bank customers loan pay bank willingness prediction.\n",
    "### Cancer tumour cells identification.\n",
    "### Sentiment analysis.\n",
    "### Drugs classification\n",
    "### Facial key points detection\n",
    "### Pedestrians detection in an automotive car driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support vector machines\n",
    "\n",
    "* SVM is related to statistical learning theory\n",
    "\n",
    "* SVM was first introduced in 1992 \n",
    "\n",
    "* SVM becomes popular because of its success in handwritten digit recognition\n",
    "\n",
    "* SVM is now regarded as an important example of “kernel methods”, one of the key area in machine learning\n",
    "\n",
    "* Note: the meaning of “kernel” is different from the “kernel” function for Parzen windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM for classification\n",
    "\n",
    "#### Support Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. \n",
    "#### However,  it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM: Mathematical Setup\n",
    "\n",
    "# A SVM constructs a hyperplane (or set of hyperplanes) in a high or infinite dimensional space, which can be used for classification, regression or other tasks\n",
    "\n",
    "![](./assest/svmplot.jpg?raw=true)\n",
    "#### Source: Digg Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why SVM work ?\n",
    "\n",
    "Pros and Cons associated with SVM\n",
    "\n",
    "    Pros:\n",
    "        It works really well with clear margin of separation\n",
    "        It is effective in high dimensional spaces.\n",
    "        It is effective in cases where number of dimensions is greater than the number of samples.\n",
    "        It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "    Cons:\n",
    "        It doesn’t perform well, when we have large data set because the required training time is higher\n",
    "        It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping\n",
    "        SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. It is related SVC method of Python scikit-learn library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical formulation\n",
    "\n",
    "### A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. \n",
    "\n",
    "### Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.\n",
    "\n",
    "\n",
    "\n",
    "Given training vectors $$ x_i \\in \\mathbb{R}^p, i=1,…, n, $$ in two classes, and a vector $$ y \\in \\{1, -1\\}^n,$$ SVC solves the following primal problem:\n",
    "\n",
    "$$\\min_ {w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i $$\n",
    "\n",
    "$$ y_i (w^T \\phi (x_i) + b) \\geq 1 - \\zeta_i, $$\n",
    "$$ \\zeta_i \\geq 0 , i=1, ..., n $$\n",
    "\n",
    "\n",
    "\n",
    "Its dual is\n",
    "\n",
    "$$ \\min_{\\alpha} \\frac{1}{2} \\alpha^T Q \\alpha - e^T \\alpha $$\n",
    " \n",
    " $$ y^T \\alpha = 0\\\\ $$ \n",
    " \n",
    " $$ 0 \\leq \\alpha_i \\leq C, i=1, ..., n $$\n",
    "\n",
    "where e is the vector of all ones, C > 0 is the upper bound, Q is an n by n positive semidefinite matrix, Q_{ij} \\equiv y_i y_j K(x_i, x_j), where $$K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)$$ is the kernel. Here training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function \\phi.\n",
    "\n",
    "The decision function is:\n",
    "\n",
    "$$ \\operatorname{sgn}(\\sum_{i=1}^n y_i \\alpha_i K(x_i, x) + \\rho) $$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SVM example \n",
    "# Support Vector Machine practice notebook with breast cancer data set\n",
    "# Import libraries and load data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Get the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 4 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "expected = digits.target[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    plt.subplot(2, 4, index + 5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Prediction: %i' % prediction)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross-validation on Digits Dataset Exercise\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "svc = svm.SVC(kernel='linear')\n",
    "C_s = np.logspace(-10, 0, 10)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "for C in C_s:\n",
    "    svc.C = C\n",
    "    this_scores = cross_val_score(svc, X, y, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "\n",
    "# Do the plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.semilogx(C_s, scores)\n",
    "plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')\n",
    "plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')\n",
    "locs, labels = plt.yticks()\n",
    "plt.yticks(locs, list(map(lambda x: \"%g\" % x, locs)))\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kernel Trick\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-Nearest Neighbhours\n",
    "\n",
    "What is KNN?\n",
    "\n",
    "\n",
    "x to denote a feature (aka. predictor, attribute) and y to denote the target (aka. label, class) we are trying to predict.\n",
    "\n",
    "Informally, this means that we are given a labelled dataset consiting of training observations (x,y)(x,y) and would like to capture the relationship between xx and yy. More formally, our goal is to learn a function h:X→Y, so that given an unseen observation xx, h(x)h(x) can confidently predict the corresponding output yy.\n",
    "\n",
    "The KNN classifier is also a non parametric and instance-based learning algorithm.\n",
    "\n",
    "Non-parametric means it makes no explicit assumptions about the functional form of h, avoiding the dangers of mismodeling the underlying distribution of the data. For example, suppose our data is highly non-Gaussian but the learning model we choose assumes a Gaussian form. In that case, our algorithm would make extremely poor predictions.\n",
    "\n",
    "Instance-based learning means that our algorithm doesn’t explicitly learn a model. \n",
    "\n",
    "Instead, it chooses to memorize the training instances which are subsequently used as “knowledge” for the prediction phase. Concretely, this means that only when a query to our database is made (i.e. when we ask it to predict a label given an input), will the algorithm use the training instances to spit out an answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mathematical formulation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k-Nearest Neighbors Classification\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "n_neighbors = 15\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pros and Cons of K-NN approach\n",
    "\n",
    "## Pros\n",
    "\n",
    "### As you can already tell from the previous section, one of the most attractive features of the K-nearest neighbor algorithm is that is simple to understand and easy to implement. \n",
    "\n",
    "### With zero to little training time, it can be a useful tool for off-the-bat analysis of some data set you are planning to run more complex algorithms on. \n",
    "\n",
    "### Furthermore, KNN works just as easily with multiclass data sets whereas other algorithms are hardcoded for the binary setting. Finally, as we mentioned earlier, the non-parametric nature of KNN gives it an edge in certain settings where the data may be highly “unusual”.\n",
    "\n",
    "## Cons\n",
    "\n",
    "### One of the obvious drawbacks of the KNN algorithm is the computationally expensive testing phase which is impractical in industry settings. \n",
    "\n",
    "### Note the rigid dichotomy between KNN and the more sophisticated Neural Network which has a lengthy training phase albeit a very fast testing phase. Furthermore, KNN can suffer from skewed class distributions. For example, if a certain class is very frequent in the training set, it will tend to dominate the majority voting of the new example (large number = more common). \n",
    "\n",
    "### Finally, the accuracy of KNN can be severely degraded with high-dimension data because there is little difference between the nearest and farthest neighbor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Learning\n",
    "\n",
    "### For general use, decision trees are employed to visually represent decisions and show or inform decision making. When working with machine learning and data mining, decision trees are used as a predictive model. These models map observations about data to conclusions about the data’s target value.\n",
    "\n",
    "### The goal of decision tree learning is to create a model that will predict the value of a target based on input variables.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Algorithms\n",
    "\n",
    "All classification and regression algorithms come under supervised learning.\n",
    "\n",
    "* Logistic Regression\n",
    "* Decision trees\n",
    "* Support vector machine (SVM)\n",
    "* k-Nearest Neighbors\n",
    "* Naive Bayes\n",
    "* Random forest\n",
    "* Linear regression\n",
    "* Polynomial regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification algorithms\n",
    "\n",
    "#### Linear classifiers\n",
    "\n",
    "#### Support vector machines (SVMs)\n",
    "\n",
    "#### k-Nearest Neighbour\n",
    "\n",
    "#### Logistic regression\n",
    "\n",
    "#### Naive Bayes classifiers\n",
    "\n",
    "#### Fisher’s linear discriminant\n",
    "\n",
    "#### Quadratic classifiers\n",
    "\n",
    "#### Kernel estimation\n",
    "\n",
    "#### k-nearest neighbor\n",
    "\n",
    "#### Decision trees\n",
    "\n",
    "#### Random forests\n",
    "\n",
    "#### Learning vector quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised learning:\n",
    "\n",
    "## Task of inferring a function to describe hidden structure from \"unlabeled\" data \n",
    "\n",
    "### Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "### 1. Gaussian mixture models\n",
    "\n",
    "### 2. Manifold learning\n",
    "\n",
    "### 3. Covariance estimation\n",
    "\n",
    "### 4. Novelty and Outlier Detection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning:\n",
    "\n",
    "All clustering algorithms come under unsupervised learning algorithms.\n",
    "\n",
    "* K-means clustering\n",
    "* Hierarchical clustering\n",
    "* Hidden Markov models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning: Cluster Analysis\n",
    "\n",
    "### Clustering: The assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis used in many fields.\n",
    "\n",
    "### K-means Clustering\n",
    "\n",
    "### Other Clustering algorithms:\n",
    "\n",
    "       ### Exclusive Clustering\n",
    "       ### Overlapping Clustering\n",
    "       ### Hierarchical Clustering\n",
    "       ### Probabilistic Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding of Clustering\n",
    "\n",
    "Typical cluster models include:\n",
    "\n",
    "    Connectivity models: for example, hierarchical clustering builds models based on distance connectivity.\n",
    "    \n",
    "    Centroid models: for example, the k-means algorithm represents each cluster by a single mean vector.\n",
    "    \n",
    "    Distribution models: clusters are modeled using statistical distributions, such as multivariate normal distributions used by the expectation-maximization algorithm.\n",
    "    Density models: for example, DBSCAN and OPTICS defines clusters as connected dense regions in the data space.\n",
    "    Subspace models: in biclustering (also known as co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes.\n",
    "    Group models: some algorithms do not provide a refined model for their results and just provide the grouping information.\n",
    "    Graph-based models: a clique, that is, a subset of nodes in a graph such that every two nodes in the subset are connected by an edge can be considered as a prototypical form of cluster. Relaxations of the complete connectivity requirement (a fraction of the edges can be missing) are known as quasi-cliques, as in the HCS clustering algorithm.\n",
    "    Neural models: the most well known unsupervised neural network is the self-organizing map and these models can usually be characterized as similar to one or more of the above models, and including subspace models when neural networks implement a form of Principal Component Analysis or Independent Component Analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goal of Clustering\n",
    "\n",
    "## The goal of clustering is to determine the intrinsic grouping in a set of unlabeled data.\n",
    "\n",
    "But how does one decide what constitutes a good clustering?\n",
    "\n",
    "It can be shown that there is no absolute “best” criterion which would\n",
    "be independent of the final aim of the clustering. Consequently, it is\n",
    "the user which must supply this criterion, in such a way that the result\n",
    "of the clustering will suit their needs\n",
    "\n",
    "Unsupervised learning is often used to preprocess the data. Usually, that means compressing it in some meaning-preserving way like with PCA or SVD before feeding it to a deep neural net or another supervised learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical understanding of K-means algorithm \n",
    "\n",
    "#### An important component of a clustering algorithm is the distance measure between data points. If the components of the data instance vectors are all in the same physical units then it is possible that the simple Euclidean distance metric is sufficient to successfully group similar data instances. \n",
    "\n",
    "\n",
    "$$\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_j - \\mu_i||^2)$$\n",
    "\n",
    "#### Inertia, or the within-cluster sum of squares criterion, can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:\n",
    "\n",
    "    Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.\n",
    "    Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as PCA prior to k-means clustering can alleviate this problem and speed up the computations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-Means algorithm\n",
    "\n",
    "K-means is often referred to as Lloyd’s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose k samples from the dataset X. After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.\n",
    "../_images/sphx_glr_plot_kmeans_digits_0011.png\n",
    "\n",
    "K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.\n",
    "\n",
    "The algorithm can also be understood through the concept of Voronoi diagrams. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other approaches\n",
    "\n",
    "### Hierarchical clustering and Fuzzy C-means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "n_digits: 10, \t n_samples 1797, \t n_features 64\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhouette\n",
      "k-means++\t1.41s\t69432\t0.602\t0.650\t0.625\t0.465\t0.598\t0.146\n",
      "random   \t0.35s\t69694\t0.669\t0.710\t0.689\t0.553\t0.666\t0.147\n",
      "PCA-based\t0.05s\t70804\t0.671\t0.698\t0.684\t0.561\t0.668\t0.118\n",
      "__________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njIxMA0IWfRkZGZkGxP8DhGQcDUxW1VsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faac4f85fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clustering example from scikit learn\n",
    "# A demo of K-Means clustering on the handwritten digits data\n",
    "\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "digits = load_digits()\n",
    "data = scale(digits.data)\n",
    "\n",
    "n_samples, n_features = data.shape\n",
    "n_digits = len(np.unique(digits.target))\n",
    "labels = digits.target\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
    "      % (n_digits, n_samples, n_features))\n",
    "\n",
    "\n",
    "print(82 * '_')\n",
    "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "\n",
    "def bench_k_means(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "\n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "              name=\"k-means++\", data=data)\n",
    "\n",
    "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
    "              name=\"random\", data=data)\n",
    "\n",
    "# in this case the seeding of the centers is deterministic, hence we run the\n",
    "# kmeans algorithm only once with n_init=1\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "              name=\"PCA-based\",\n",
    "              data=data)\n",
    "print(82 * '_')\n",
    "\n",
    "# #############################################################################\n",
    "# Visualize the results on PCA-reduced data\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Application of Clustering Algorithms\n",
    "\n",
    "* Recommender systems\n",
    "\n",
    "* Anomaly detection\n",
    "\n",
    "* Human genetic clustering and Genome Sequence analysis\n",
    "\n",
    "* Analysis of antimicrobial activity\n",
    "\n",
    "* Grouping of shopping items\n",
    "\n",
    "Search result grouping\n",
    "\n",
    "Slippy map optimization\n",
    "\n",
    "Crime analysis\n",
    "\n",
    "Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML at a scale\n",
    "\n",
    "### Sharding of data and the empirical distributions of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests or random decision forests[1][2] are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Probabilistic classifiers\n",
    "\n",
    "###  A probability distribution over a set of classes, rather than only outputting the most likely class that the observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem solving using Machine learning\n",
    "\n",
    "### Classification problem\n",
    "### Anomaly detection\n",
    "### Regression \n",
    "### Clustering\n",
    "### Reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Validation of Machine learning algorithms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Two of the most popular strategies to perform the validation step are the hold-out strategy and the k-fold strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation methods\n",
    "\n",
    "\n",
    "3.1. Cross-validation: evaluating estimator performance\n",
    "\n",
    "    3.1.1. Computing cross-validated metrics\n",
    "        3.1.1.1. The cross_validate function and multiple metric evaluation\n",
    "        3.1.1.2. Obtaining predictions by cross-validation\n",
    "    3.1.2. Cross validation iterators\n",
    "        3.1.2.1. Cross-validation iterators for i.i.d. data\n",
    "            3.1.2.1.1. K-fold\n",
    "            3.1.2.1.2. Repeated K-Fold\n",
    "            3.1.2.1.3. Leave One Out (LOO)\n",
    "            3.1.2.1.4. Leave P Out (LPO)\n",
    "            3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split\n",
    "        3.1.2.2. Cross-validation iterators with stratification based on class labels.\n",
    "            3.1.2.2.1. Stratified k-fold\n",
    "            3.1.2.2.2. Stratified Shuffle Split\n",
    "        3.1.2.3. Cross-validation iterators for grouped data.\n",
    "            3.1.2.3.1. Group k-fold\n",
    "            3.1.2.3.2. Leave One Group Out\n",
    "            3.1.2.3.3. Leave P Groups Out\n",
    "            3.1.2.3.4. Group Shuffle Split\n",
    "        3.1.2.4. Predefined Fold-Splits / Validation-Sets\n",
    "        3.1.2.5. Cross validation of time series data\n",
    "            3.1.2.5.1. Time Series Split\n",
    "    3.1.3. A note on shuffling\n",
    "    3.1.4. Cross validation and model selection\n",
    "    \n",
    "  \n",
    "\n",
    "3.2. Tuning the hyper-parameters of an estimator\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "3.5. Validation curves: plotting scores to evaluate models\n",
    "\n",
    "    3.5.1. Validation curve\n",
    "    3.5.2. Learning curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting\n",
    "• Underfitting refers to a model that can\n",
    "neither model the training data not generalize\n",
    "to new data\n",
    "• An underfit machine learning model is not a\n",
    "suitable model and it will have poor\n",
    "performance on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "### A partial remedy to the problem of an overfit machine learning model using a technique known as cross-validation.\n",
    "\n",
    "\n",
    "This process of deciding whether the numerical results quantifying hypothesized relationships between variables, are acceptable as descriptions of the data, is known as validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation\n",
    "\n",
    "By reducing the training data, we risk losing important patterns/ trends in data set, which in turn increases error induced by bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K-Fold Cross Validation\n",
    "\n",
    "In some cases, there may be a large imbalance in the response variables. For example, in dataset concerning price of houses, there might be large number of houses having high price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-P-Out Cross Validation\n",
    "\n",
    "This approach leaves p data points out of training data, i.e. if there are n data points in the original sample then, n-p samples are used to train the model and p points are used as the validation set.\n",
    "\n",
    "A particular case of this method is when p = 1. This is known as Leave one out cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As size of training and test set determine each other, this leaves us with a tradeoff: pessimistic bias vs high variance.\n",
    "\n",
    "### k-fold Cross validation tackles this problem by keeping the training set large (a fraction of k−1kk−1k of the data is used for training in every iteration) and dealing with the variance of the test error by resampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation techniques in scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stratification \n",
    "\n",
    "### means that we randomly split the dataset so that each class is correctly represented in the resulting subsets — the training and the test set.\n",
    "\n",
    "Random subsampling in non-stratified fashion is usually not a big concern if we are working with relatively large and balanced datasets. However, in my opinion, stratified resampling is usually (only) beneficial in machine learning applications. Moreover, stratified sampling is incredibly easy to implement, and Ron Kohavi provides empirical evidence (Kohavi 1995) that stratification has a positive effect on the variance and bias of the estimate in k-fold cross-validation, a technique we will discuss later in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performance metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Types of errors\n",
    "\n",
    "* Type I error\n",
    "\n",
    "* Type II error\n",
    "\n",
    "* False positives\n",
    "\n",
    "* True Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Error metrics\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "### precison\n",
    "\n",
    "### Recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Receiver operating characteristic\n",
    "\n",
    "## Curves in ROC space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Z-score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ROC curves beyond binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation and Model Selection\n",
    "In this section, we'll look at model evaluation and the tuning of hyperparameters, which are parameters that define the model.\n",
    "In [1]:\n",
    "\n",
    "from __future__ import print_function, division\n",
    "​\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "​\n",
    "# Use seaborn for plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "Validating Models\n",
    "One of the most important pieces of machine learning is model validation: that is, checking how well your model fits a given dataset. But there are some pitfalls you need to watch out for.\n",
    "Consider the digits example we've been looking at previously. How might we check how well our model fits the data?\n",
    "In [2]:\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "Let's fit a K-neighbors classifier\n",
    "In [3]:\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)\n",
    "Out[3]:\n",
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_neighbors=1, p=2, weights='uniform')\n",
    "Now we'll use this classifier to predict labels for the data\n",
    "In [4]:\n",
    "\n",
    "y_pred = knn.predict(X)\n",
    "Finally, we can check how well our prediction did:\n",
    "In [5]:\n",
    "\n",
    "print(\"{0} / {1} correct\".format(np.sum(y == y_pred), len(y)))\n",
    "1797 / 1797 correct\n",
    "It seems we have a perfect classifier!\n",
    "Question: what's wrong with this?\n",
    "Validation Sets\n",
    "Above we made the mistake of testing our data on the same set of data that was used for training. This is not generally a good idea. If we optimize our estimator this way, we will tend to over-fit the data: that is, we learn the noise.\n",
    "A better way to test a model is to use a hold-out set which doesn't enter the training. We've seen this before using scikit-learn's train/test split utility:\n",
    "In [6]:\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train.shape, X_test.shape\n",
    "Out[6]:\n",
    "((1347, 64), (450, 64))\n",
    "Now we train on the training data, and validate on the test data:\n",
    "In [7]:\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"{0} / {1} correct\".format(np.sum(y_test == y_pred), len(y_test)))\n",
    "438 / 450 correct\n",
    "This gives us a more reliable estimate of how our model is doing.\n",
    "The metric we're using here, comparing the number of matches to the total number of samples, is known as the accuracy score, and can be computed using the following routine:\n",
    "In [8]:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)\n",
    "Out[8]:\n",
    "0.97333333333333338\n",
    "This can also be computed directly from the model.score method:\n",
    "In [9]:\n",
    "\n",
    "knn.score(X_test, y_test)\n",
    "Out[9]:\n",
    "0.97333333333333338\n",
    "Using this, we can ask how this changes as we change the model parameters, in this case the number of neighbors:\n",
    "In [10]:\n",
    "\n",
    "for n_neighbors in [1, 5, 10, 20, 30]:\n",
    "    knn = KNeighborsClassifier(n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(n_neighbors, knn.score(X_test, y_test))\n",
    "1 0.973333333333\n",
    "5 0.982222222222\n",
    "10 0.971111111111\n",
    "20 0.955555555556\n",
    "30 0.96\n",
    "We see that in this case, a small number of neighbors seems to be the best option.\n",
    "Cross-Validation\n",
    "One problem with validation sets is that you \"lose\" some of the data. Above, we've only used 3/4 of the data for the training, and used 1/4 for the validation. Another option is to use 2-fold cross-validation, where we split the sample in half and perform the validation twice:\n",
    "In [11]:\n",
    "\n",
    "X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "X1.shape, X2.shape\n",
    "Out[11]:\n",
    "((898, 64), (899, 64))\n",
    "In [12]:\n",
    "\n",
    "print(KNeighborsClassifier(1).fit(X2, y2).score(X1, y1))\n",
    "print(KNeighborsClassifier(1).fit(X1, y1).score(X2, y2))\n",
    "0.983296213808\n",
    "0.982202447164\n",
    "Thus a two-fold cross-validation gives us two estimates of the score for that parameter.\n",
    "Because this is a bit of a pain to do by hand, scikit-learn has a utility routine to help:\n",
    "In [13]:\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cv = cross_val_score(KNeighborsClassifier(1), X, y, cv=10)\n",
    "cv.mean()\n",
    "Out[13]:\n",
    "0.97614938602520218\n",
    "K-fold Cross-Validation\n",
    "Here we've used 2-fold cross-validation. This is just one specialization of KK-fold cross-validation, where we split the data into KK chunks and perform KK fits, where each chunk gets a turn as the validation set. We can do this by changing the cv parameter above. Let's do 10-fold cross-validation:\n",
    "In [14]:\n",
    "\n",
    "cross_val_score(KNeighborsClassifier(1), X, y, cv=10)\n",
    "Out[14]:\n",
    "array([ 0.93513514,  0.99453552,  0.97237569,  0.98888889,  0.96089385,\n",
    "        0.98882682,  0.99441341,  0.98876404,  0.97175141,  0.96590909])\n",
    "This gives us an even better idea of how well our model is doing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overfitting, Underfitting, and Model Selection\n",
    "\n",
    "Now that we've gone over the basics of validation, and cross-validation, it's time to go into even more depth regarding model selection.\n",
    "\n",
    "The issues associated with validation and cross-validation are some of the most important aspects of the practice of machine learning. Selecting the optimal model for your data is vital, and is a piece of the problem that is not often appreciated by machine learning practitioners.\n",
    "\n",
    "Of core importance is the following question: If our estimator is underperforming, how should we move forward?\n",
    "Use simpler or more complicated model?\n",
    "\n",
    "Add more features to each observed data point?\n",
    "\n",
    "Add more training samples?\n",
    "\n",
    "The answer is often counter-intuitive. In particular, Sometimes using a more complicated model will give worse results. Also, Sometimes adding training data will not improve your results. The ability to determine what steps will improve your model is what separates the successful machine learning practitioners from the unsuccessful.\n",
    "Illustration of the Bias-Variance Tradeoff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "When we increase the degree to this extent, it's clear that the resulting fit is no longer reflecting the true underlying distribution, but is more sensitive to the noise in the training data. For this reason, we call it a high-variance model, and we say that it over-fits the data.\n",
    "Just for fun, let's use IPython's interact capability (only in IPython 2.0+) to explore this interactively:\n",
    "In [22]:\n",
    "\n",
    "from IPython.html.widgets import interact\n",
    "​\n",
    "def plot_fit(degree=1, Npts=50):\n",
    "    X, y = make_data(Npts, error=1)\n",
    "    X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "    \n",
    "    model = PolynomialRegression(degree=degree)\n",
    "    model.fit(X, y)\n",
    "    y_test = model.predict(X_test)\n",
    "​\n",
    "    plt.scatter(X.ravel(), y)\n",
    "    plt.plot(X_test.ravel(), y_test)\n",
    "    plt.ylim(-4, 14)\n",
    "    plt.title(\"mean squared error: {0:.2f}\".format(mean_squared_error(model.predict(X), y)))\n",
    "    \n",
    "interact(plot_fit, degree=[1, 30], Npts=[2, 100]);\n",
    "\n",
    "Detecting Over-fitting with Validation Curves\n",
    "Clearly, computing the error on the training data is not enough (we saw this previously). As above, we can use cross-validation to get a better handle on how the model fit is working.\n",
    "Let's do this here, again using the validation_curve utility. To make things more clear, we'll use a slightly larger dataset:\n",
    "In [23]:\n",
    "\n",
    "X, y = make_data(120, error=1.0)\n",
    "plt.scatter(X, y);\n",
    "\n",
    "In [24]:\n",
    "\n",
    "from sklearn.learning_curve import validation_curve\n",
    "​\n",
    "def rms_error(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "​\n",
    "degree = np.arange(0, 18)\n",
    "val_train, val_test = validation_curve(PolynomialRegression(), X, y,\n",
    "                                       'polynomialfeatures__degree', degree, cv=7,\n",
    "                                       scoring=rms_error)\n",
    "Now let's plot the validation curves:\n",
    "In [25]:\n",
    "\n",
    "def plot_with_err(x, data, **kwargs):\n",
    "    mu, std = data.mean(1), data.std(1)\n",
    "    lines = plt.plot(x, mu, '-', **kwargs)\n",
    "    plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n",
    "                     facecolor=lines[0].get_color(), alpha=0.2)\n",
    "​\n",
    "plot_with_err(degree, val_train, label='training scores')\n",
    "plot_with_err(degree, val_test, label='validation scores')\n",
    "plt.xlabel('degree'); plt.ylabel('rms error')\n",
    "plt.legend();\n",
    "\n",
    "Notice the trend here, which is common for this type of plot.\n",
    "For a small model complexity, the training error and validation error are very similar. This indicates that the model is under-fitting the data: it doesn't have enough complexity to represent the data. Another way of putting it is that this is a high-bias model.\n",
    "As the model complexity grows, the training and validation scores diverge. This indicates that the model is over-fitting the data: it has so much flexibility, that it fits the noise rather than the underlying trend. Another way of putting it is that this is a high-variance model.\n",
    "Note that the training score (nearly) always improves with model complexity. This is because a more complicated model can fit the noise better, so the model improves. The validation data generally has a sweet spot, which here is around 5 terms.\n",
    "Here's our best-fit model according to the cross-validation:\n",
    "In [26]:\n",
    "\n",
    "model = PolynomialRegression(4).fit(X, y)\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X_test, model.predict(X_test));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# For this section, we'll work with a simple 1D regression problem. This will help us to easily visualize the data and the model, and the results generalize easily to higher-dimensional datasets. \n",
    "\n",
    "\n",
    "# We'll explore a simple linear regression problem. This can be accomplished within scikit-learn with the sklearn.linear_model module.\n",
    "# We'll create a simple nonlinear function that we'd like to fit\n",
    "\n",
    "\n",
    "def test_func(x, err=0.5):\n",
    "    y = 10 - 1. / (x + 0.1)\n",
    "    if err > 0:\n",
    "        y = np.random.normal(y, err)\n",
    "    return y\n",
    "# Now let's create a realization of this dataset:\n",
    "\n",
    "def make_data(N=40, error=1.0, random_seed=1):\n",
    "    # randomly sample the data\n",
    "    np.random.seed(1)\n",
    "    X = np.random.random(N)[:, np.newaxis]\n",
    "    y = test_func(X.ravel(), error)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detecting Data Sufficiency with Learning Curves\n",
    "\n",
    "As you might guess, the exact turning-point of the tradeoff between bias and variance is highly dependent on the number of training points used. Here we'll illustrate the use of learning curves, which display this property.\n",
    "The idea is to plot the mean-squared-error for the training and test set as a function of Number of Training Points\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.learning_curve import learning_curve\n",
    "​\n",
    "def plot_learning_curve(degree=3):\n",
    "    train_sizes = np.linspace(0.05, 1, 20)\n",
    "    N_train, val_train, val_test = learning_curve(PolynomialRegression(degree),\n",
    "                                                  X, y, train_sizes, cv=5,\n",
    "                                                  scoring=rms_error)\n",
    "    plot_with_err(N_train, val_train, label='training scores')\n",
    "    plot_with_err(N_train, val_test, label='validation scores')\n",
    "    plt.xlabel('Training Set Size'); plt.ylabel('rms error')\n",
    "    plt.ylim(0, 3)\n",
    "    plt.xlim(5, 80)\n",
    "    plt.legend()\n",
    "Let's see what the learning curves look like for a linear model:\n",
    "\n",
    "\n",
    "plot_learning_curve(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This shows a typical learning curve: for very few training points, there is a large separation between the training and test error, which indicates over-fitting. Given the same model, for a large number of training points, the training and testing errors converge, which indicates potential under-fitting.\n",
    "As you add more data points, the training error will never increase, and the testing error will never decrease (why do you think this is?)\n",
    "It is easy to see that, in this plot, if you'd like to reduce the MSE down to the nominal value of 1.0 (which is the magnitude of the scatter we put in when constructing the data), then adding more samples will never get you there. For d=1d=1, the two curves have converged and cannot move lower. What about for a larger value of dd?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For an even more complex model, we still converge, but the convergence only happens for large amounts of training data.\n",
    "So we see the following:\n",
    "you can cause the lines to converge by adding more points or by simplifying the model.\n",
    "you can bring the convergence error down only by increasing the complexity of the model.\n",
    "Thus these curves can give you hints about how you might improve a sub-optimal model. If the curves are already close together, you need more model complexity. If the curves are far apart, you might also improve the model by adding more data.\n",
    "To make this more concrete, imagine some telescope data in which the results are not robust enough. You must think about whether to spend your valuable telescope time observing more objects to get a larger training set, or more attributes of each object in order to improve the model. The answer to this question has real consequences, and can be addressed using these metrics.\n",
    "Summary\n",
    "We've gone over several useful tools for model validation\n",
    "The Training Score shows how well a model fits the data it was trained on. This is not a good indication of model effectiveness\n",
    "The Validation Score shows how well a model fits hold-out data. The most effective method is some form of cross-validation, where multiple hold-out sets are used.\n",
    "Validation Curves are a plot of validation score and training score as a function of model complexity:\n",
    "when the two curves are close, it indicates underfitting\n",
    "when the two curves are separated, it indicates overfitting\n",
    "the \"sweet spot\" is in the middle\n",
    "Learning Curves are a plot of the validation score and training score as a function of Number of training samples\n",
    "when the curves are close, it indicates underfitting, and adding more data will not generally improve the estimator.\n",
    "when the curves are far apart, it indicates overfitting, and adding more data may increase the effectiveness of the model.\n",
    "These tools are powerful means of evaluating your model on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model evaluation procedures\n",
    "\n",
    "Training and testing on the same data\n",
    "\n",
    "Rewards overly complex models that \"overfit\" the training data and won't necessarily generalise\n",
    "\n",
    "Train/test split: Split the dataset into two pieces, so that the model can be trained and tested on different data\n",
    "Better estimate of out-of-sample performance, but still a \"high variance\" estimate\n",
    "Useful due to its speed, simplicity, and flexibility\n",
    "K-fold cross-validation\n",
    "Systematically create \"K\" train/test splits and average the results together\n",
    "Even better estimate of out-of-sample performance\n",
    "Runs \"K\" times slower than train/test split\n",
    "Model evaluation metrics\n",
    "Regression problems: Mean Absolute Error, Mean Squared Error, Root Mean Squared Error\n",
    "Classification problems: Classification accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "\n",
    "## Supervised and Unsupervised learning\n",
    "\n",
    "* Classification algorithms\n",
    " \n",
    "* Clustering algorithms\n",
    "\n",
    "* Recommendation systems\n",
    "\n",
    "\n",
    "![](./assest/mlalg.png?raw=true)\n",
    "#### Source: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification algorithms\n",
    "\n",
    "* Decision trees\n",
    "* Support vector machines\n",
    "* Neural networks\n",
    "* Kernel methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clustering algorithms\n",
    "\n",
    "* connectivity, density, subspace, group, neural models\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recommendation systems\n",
    "\n",
    "* Collaborative filtering\n",
    "* Content-based filtering\n",
    "* Hybrid recommender systems\n",
    "* Opinion-based recommender systems\n",
    "* Multi-criteria recommender system\n",
    "\n",
    "### Netflix prize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem solving using Machine learning\n",
    "\n",
    "### Classification problem\n",
    "### Anomaly detection\n",
    "### Regression \n",
    "### Clustering\n",
    "### Reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 Popular Algorithms\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
